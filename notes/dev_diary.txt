7/31

Hey, here's a thing I wish I had started earlier.

Currently trying to recover state on latest rnn runs (which were a while ago).

Based on a sketchy comparison of f-scores, it seems like the new models (trained for up to 24k iters)
are worse than the ones trained earlier (pre data pipeline refactor).

I'm guessing the reason for this is that the latest runs go through the full training set indiscriminately,
whereas the previous (feed_dict) implementation sampled a vector per user.

I've tried implementing something similar using tf.contrib.data.rejection_resample, but it is **slow**.
So much slower. Like 10x. :(

Maybe the solution is to start it off training on the full training set, then do a short 'fine-tuning' phase
on a sampled subset?

Alternative: maybe you can implement a more performant alternative to rejection_resample for your use case?

8/3

Goal: get a sense of where different RNN models are in terms of f-score. See if we've actually improved since the models from a month ago (god I hope so).

Validation/test sets are identical across machines, so should be a fair comparison. May have some issues getting older checkpoints off the ground (different features, etc.)
May need to check out some earlier commits to do inference on some of these older  models? Bleh.

Using threshold predictor rather than optimizer per-user thresholds is about 100x faster, and should at least be a fair basis of comparison.
