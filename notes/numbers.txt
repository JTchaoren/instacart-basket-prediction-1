200k users (206,209 to be exact)
current split:
  test.tfrecords: 509
  train.tfrecords: 195795
  eval.tfrecords: 9905
Kaggle-defined test set:
  - 75k users
  - 45,100 batches = 4,510,000 sequences
50k products
3.4m orders
# of prior orders ranges from 3 to 100, inclusive
products / order ranges from 1 to around 55. mode =5.

dist. of # of products per user
  see poke_probs.py
  out of 509 test users...
  mean = 65
  min = 1(?)
  25% = 26
  median = 46
  75% = 87
  max = 200-something?

dist. of # of orders per user
  mean = 16
  min = 3
  25% = 6
  median = 10
  75% = 20
  max = 100

Some fun stuff from poke_predictions.py...
Calculated over 4810 predictions for 509 test users
(These are stats describing predicted products)
- dist of orders_since:
  mean = 1.7
  median = 1
  75% = 2
  82% = 3, 95% = 4
  max = 10
- dist of nprev:
  mean = 6.3
  min = 1
  10% = 1
  25% = 2
  median = 4
  75% = 8
  max = 81

Some timing stats:
- 1s/batch for training (batch_size=100)
- About 25% of that time is spent computing the input vectors 
  - (Also, some large but unknown % is spent copying data from python runtime to tf runtime)
- 110s to calculate validation loss (so calculating this every 1.5k batches, means it 
  accounts for around 5-10% of training time)
- precomputing probabilities for 509 test users: 2min
- eval.py: 20 min on 509 users
  - which is pretty slow. worth profiling. Maybe have a 'lazy mode' that cuts a few corners
    for big speedups (e.g. in terms of trying fewer thresholds)
      - also maaaaybe worth considering trying to turn O(n^4) E[f] algo into O(n^3)
- caching lookups (oid_to_pids etc.): 800s
- generating user pbs: 20min
- precomputing vectors: 95s for 9905 users, using prod limit of 5 (626,532 seqs)
