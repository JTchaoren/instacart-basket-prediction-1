200k users (206,209 to be exact)
current split:
  test.tfrecords: 509
  train.tfrecords: 195795
  eval.tfrecords: 9905
Kaggle-defined test set:
  - 75k users
  - 45,100 batches = 4,510,000 sequences
50k products
3.4m orders
# of prior orders ranges from 3 to 100, inclusive
products / order ranges from 1 to around 55. mode =5.

dist. of # of products per user
  see poke_probs.py
  out of 509 test users...
  mean = 65
  min = 1(?)
  25% = 26
  median = 46
  75% = 87
  max = 200-something?

dist. of # of orders per user
  mean = 16
  min = 3
  25% = 6
  median = 10
  75% = 20
  max = 100

Some fun stuff from poke_predictions.py...
Calculated over 4810 predictions for 509 test users
(These are stats describing predicted products)
- dist of orders_since:
  mean = 1.7
  median = 1
  75% = 2
  82% = 3, 95% = 4
  max = 10
- dist of nprev:
  mean = 6.3
  min = 1
  10% = 1
  25% = 2
  median = 4
  75% = 8
  max = 81

Some timing stats:
- 1s/batch for training (batch_size=100)
- About 25% of that time is spent computing the input vectors 
  - (Also, some large but unknown % is spent copying data from python runtime to tf runtime)
- 110s to calculate validation loss (so calculating this every 1.5k batches, means it 
  accounts for around 5-10% of training time)
- precomputing probabilities:
  - 509 test users -> 2min
- eval.py: 20 min on 509 users
  - which is pretty slow. worth profiling. Maybe have a 'lazy mode' that cuts a few corners
    for big speedups (e.g. in terms of trying fewer thresholds)
      - also maaaaybe worth considering trying to turn O(n^4) E[f] algo into O(n^3)
  - 5min with '--quick' mode (lower thresh for using hybrid predictor, non-exhaustive 
    enumeration of cand thresholds)
- caching lookups (oid_to_pids etc.): 800s
- generating user pbs: 20min
- vectorizing user pbs:
  - 8.3k users (validation set), 5 prods/usr, 41k seqs => 54s (was 45s before n_prev_re(orders|peats))
  - 95s for 9905 users, using prod limit of 5
  - 20min for train set set (196k users) with prodlim of 5 
  - ~5hrs (20k secs) for train set with no product lim
    - *only* 670MB! Not bad.
    - 12B sequences
- 'scalar vectorization':
  - 3s for 100 users (no product limit)
  - 1min for test set
  - not bad!
